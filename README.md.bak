# ğŸš€ Transformer-Powered Neural Machine Translation Engine

> **Enterprise-Grade Machine Translation System** featuring a production-ready Transformer architecture with comprehensive MLOps infrastructure, end-to-end testing, and cloud-native deployment capabilities.

<div align="center">

[![Python 3.11+](https://img.shields.io/badge/Python-3.11%2B-blue?style=flat-square&logo=python)](https://www.python.org/)
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0%2B-red?style=flat-square&logo=pytorch)](https://pytorch.org/)
[![Flask](https://img.shields.io/badge/Flask-3.0+-green?style=flat-square&logo=flask)](https://flask.palletsprojects.com/)
[![Apache Airflow](https://img.shields.io/badge/Airflow-2.8+-017cee?style=flat-square&logo=apache-airflow)](https://airflow.apache.org/)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED?style=flat-square&logo=docker)](https://www.docker.com/)
[![Tests](https://img.shields.io/badge/Tests-Comprehensive-success?style=flat-square)](tests/)
[![License](https://img.shields.io/badge/License-MIT-green.svg?style=flat-square)](LICENSE)

![Transformer Architecture](https://img.shields.io/badge/Status-Production%20Ready-brightgreen?style=flat-square)

</div>

## ï¿½ Table of Contents

| Section | Details |
|---------|---------|
| [ğŸ¯ Overview](#-overview) | Project description & vision |
| [âœ¨ Key Features](#-key-features) | Technical highlights |
| [ğŸ—ï¸ Architecture](#-architecture) | System design & components |
| [ğŸš€ Quick Start](#-quick-start) | Get up & running in 5 minutes |
| [ğŸ“Š Performance](#-performance) | Benchmarks & results |
| [ğŸ”§ Advanced Usage](#-advanced-usage) | Deep dive into features |
| [ğŸ³ Deployment](#-deployment) | Production deployment guide |
| [ğŸ§ª Testing](#-testing-suite) | Test coverage & CI/CD |
| [ğŸ“š Documentation](#-documentation) | Full API reference |
| [ğŸ¤ Contributing](#-contributing) | How to contribute |

---

## ğŸ¯ Overview

A **state-of-the-art Neural Machine Translation system** implementing the Transformer architecture from scratch, complete with:

- **ğŸ”¬ From-Scratch Implementation** - Built entirely without high-level abstractions for deep learning
- **ğŸŒ Multilingual Support** - English â†’ Spanish/Hindi translation with extensible architecture  
- **âš¡ Production-Grade Infrastructure** - Enterprise-ready with Docker, Airflow, MLflow
- **ğŸ“ˆ Full MLOps Pipeline** - Experiment tracking, automated training, model versioning
- **ğŸ¯ Comprehensive Testing** - Unit tests, integration tests, model validation
- **ğŸš€ Cloud-Ready** - Containerized, scalable, orchestrated deployment

---

## âœ¨ Key Features

### ğŸ§  Core ML Components
| Feature | Specification |
|---------|---------------|
| **Architecture** | Transformer (Multi-Head Attention, 8 heads) |
| **Training Data** | 93K+ parallel sentences from Opus Books |
| **Tokenization** | SentencePiece with vocabulary optimization |
| **Languages** | English, Spanish, Hindi (extensible) |
| **Model Size** | 128D embeddings, 3-layer encoder/decoder |
| **Optimization** | Adam with learning rate scheduling |

### ğŸ› ï¸ Software Engineering
- **Modular Design** - Loosely-coupled, reusable components
- **Pipeline Architecture** - Separate training & inference pipelines
- **Exception Handling** - Custom exceptions with detailed context
- **Structured Logging** - JSON-formatted logs with multiple handlers
- **Type Hints** - Full type annotations for IDE support
- **Configuration Management** - YAML-based hyperparameter control

### ğŸš€ MLOps & DevOps
- **Experiment Tracking** - MLflow integration for all runs
- **Workflow Orchestration** - Apache Airflow for automated pipelines
- **CI/CD Pipelines** - GitHub Actions for continuous integration
- **Model Versioning** - DVC for reproducibility
- **Containerization** - Multi-stage Docker builds
- **Monitoring** - TensorBoard + custom metrics

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Installation

```bash
# Clone the repository
git clone https://github.com/Amit95688/Transformer-From-Scratch.git
cd Transformer-From-Scratch

# Create virtual environment (Python 3.14+)
python3 -m venv .venv

# Activate environment
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
.venv/bin/pip install --upgrade pip
.venv/bin/pip install torch datasets tokenizers tqdm tensorboard Flask
```

### 2ï¸âƒ£ Train the Model

```bash
# Using the new modular training pipeline
.venv/bin/python src/TransformerModel/pipelines/training_pipeline.py

# Or use the original training script
.venv/bin/python scripts/train.py
```

### 3ï¸âƒ£ Run Web Application

```bash
.venv/bin/python app.py
# Visit http://localhost:5000
```

### 4ï¸âƒ£ Test Components

```bash
.venv/bin/python test.py
```

---

## ğŸ“ Project Structure

```
transformer/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ TransformerModel/              # Main package
â”‚       â”œâ”€â”€ components/                # Core components
â”‚       â”‚   â”œâ”€â”€ data_ingestion.py      # Dataset loading
â”‚       â”‚   â”œâ”€â”€ data_transformation.py # Tokenization & preprocessing
â”‚       â”‚   â”œâ”€â”€ model_trainer.py       # Transformer architecture
â”‚       â”‚   â””â”€â”€ model_evaluation.py    # Evaluation metrics
â”‚       â”‚
â”‚       â”œâ”€â”€ pipelines/                 # End-to-end workflows
â”‚       â”‚   â”œâ”€â”€ training_pipeline.py   # Training workflow
â”‚       â”‚   â””â”€â”€ prediction_pipeline.py # Inference workflow
â”‚       â”‚
â”‚       â”œâ”€â”€ utils/                     # Utilities
â”‚       â”‚   â”œâ”€â”€ utils.py               # Helper functions
â”‚       â”‚   â”œâ”€â”€ logger.py              # Logging utilities
â”‚       â”‚   â””â”€â”€ metrics.py             # Metrics collection
â”‚       â”‚
â”‚       â”œâ”€â”€ logger.py                  # Logging configuration
â”‚       â””â”€â”€ exception.py               # Custom exceptions
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py                      # Hyperparameters & settings
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ tokenizers/                    # Trained tokenizers
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ research.ipynb                 # Experiments & research
â”‚
â”œâ”€â”€ templates/                         # HTML templates
â”‚   â”œâ”€â”€ index.html                     # Home page
â”‚   â””â”€â”€ translate.html                 # Translation UI
â”‚
â”œâ”€â”€ tests/                             # Unit tests
â”œâ”€â”€ scripts/                           # Training scripts
â”œâ”€â”€ models/                            # Saved models & checkpoints
â”‚
â”œâ”€â”€ app.py                             # Flask web application
â”œâ”€â”€ test.py                            # Component testing
â”œâ”€â”€ setup.py                           # Package installation
â”œâ”€â”€ requirements.txt                   # Dependencies
â””â”€â”€ README.md                          # This file
```

---

## âš™ï¸ Installation

### Prerequisites
- Python 3.10+
- 4GB+ RAM (8GB+ recommended for training)
- GPU support (CUDA 12.1+) optional but recommended

### Setup Steps

```bash
# 1. Clone repository
git clone <repo-url>
cd transformer

# 2. Create virtual environment
python3 -m venv .venv

# 3. Activate (Linux/Mac)
source .venv/bin/activate
# Or on Windows:
# .venv\Scripts\activate

# 4. Install minimal dependencies
pip install --upgrade pip
pip install -r requirements_minimal.txt

# 5. Install as editable package (optional)
pip install -e .
```

---

## ğŸ¯ Usage

### Training the Model

```bash
# Run training pipeline
python src/TransformerModel/pipelines/training_pipeline.py
```

**Configuration** (edit `config/config.py`):
- `batch_size`: Batch size for training (default: 2)
- `num_epochs`: Number of training epochs (default: 3)
- `lr`: Learning rate (default: 0.0001)
- `seq_length`: Maximum sequence length (default: 128)
- `d_model`: Model dimension (default: 128)
- `nhead`: Number of attention heads (default: 8)

### Prediction

```python
from TransformerModel.pipelines.prediction_pipeline import PredictPipeline, CustomData

# Initialize pipeline
predictor = PredictPipeline(
    model_path='models/model.pth',
    tokenizer_src_path='data/tokenizers/tokenizer_en.json',
    tokenizer_tgt_path='data/tokenizers/tokenizer_es.json'
)

# Translate text
result = predictor.predict("Hello, how are you?")
print(result)
```

### Web Interface

```bash
python app.py
# Open browser to http://localhost:5000
```

---

## ğŸ‹ï¸ Training

### Quick Training Run

```bash
# Minimal setup for testing (small dataset, 1 epoch)
python src/TransformerModel/pipelines/training_pipeline.py
```

### Monitor Training

```bash
# In another terminal, start TensorBoard
tensorboard --logdir=runs/
# Open http://localhost:6006
```

### Training Outputs
- Model checkpoints: `models/runs/`
- Logs: `logs/`
- Tensorboard events: `runs/`

### Expected Training Time
- 1 epoch on CPU: ~2-3 hours
- 1 epoch on GPU: ~10-15 minutes

---

## ğŸ“š API Documentation

### Training Pipeline

```python
from TransformerModel.pipelines.training_pipeline import TrainingPipeline

pipeline = TrainingPipeline()
pipeline.start()  # Starts training
```

### Prediction Pipeline

```python
from TransformerModel.pipelines.prediction_pipeline import PredictPipeline, CustomData

pipeline = PredictPipeline(
    model_path='path/to/model.pth',
    tokenizer_src_path='path/to/src_tokenizer.json',
    tokenizer_tgt_path='path/to/tgt_tokenizer.json',
    device='cpu'  # or 'cuda'
)

translation = pipeline.predict("Input text here")
```

### Components

#### DataIngestion
```python
from TransformerModel.components.data_ingestion import DataIngestion

data_ingestion = DataIngestion()
train_path, test_path = data_ingestion.initiate_data_ingestion(
    datasource='Helsinki-NLP/opus_books',
    lang_pair='translation'
)
```

#### BilingualDataset
```python
from TransformerModel.components.data_transformation import BilingualDataset

dataset = BilingualDataset(
    ds=raw_dataset,
    tokenizer_src=src_tokenizer,
    tokenizer_tgt=tgt_tokenizer,
    source_lang='en',
    target_lang='es',
    seq_length=128
)
```

---

## ğŸ³ Docker Deployment

```bash
# Build image
docker build -t transformer-model .

# Run container
docker run -p 5000:5000 transformer-model

# Docker Compose
docker-compose up
```

---

## ğŸ§ª Testing

```bash
# Run all tests
python test.py

# Or pytest
pip install pytest
pytest tests/ -v
```

---

## ğŸ“ Configuration Reference

Edit `config/config.py` to customize:

```python
{
    "batch_size": 2,              # Training batch size
    "num_epochs": 3,              # Number of epochs
    "lr": 0.0001,                 # Learning rate
    "seq_length": 128,            # Sequence length
    "d_model": 128,               # Model dimension
    "nhead": 8,                   # Number of attention heads
    "num_encoder_layers": 3,      # Encoder layers
    "num_decoder_layers": 3,      # Decoder layers
    "dim_feedforward": 256,       # FFN dimension
    "dropout": 0.1,               # Dropout rate
    "datasource": "Helsinki-NLP/opus_books",  # Dataset
    "lang_src": "en",             # Source language
    "lang_tgt": "es",             # Target language
}
```

---

## ğŸ”§ Troubleshooting

### ModuleNotFoundError
```bash
# Ensure you're in the project root and .venv is activated
source .venv/bin/activate
python src/TransformerModel/pipelines/training_pipeline.py
```

### Memory Issues
```python
# Reduce batch size in config/config.py
"batch_size": 1  # Instead of 2
```

### Slow Training
- Use GPU: Set `device='cuda'` in code
- Reduce sequence length: Change `seq_length` to 64
- Reduce dataset size: Modify `num_epochs`

---

## ğŸ“Š Current Status

âœ… **Completed**
- Modular project structure
- Training pipeline with full implementation
- Prediction pipeline
- Web application (Flask)
- Logging and exception handling
- Configuration management
- Dataset loading (HuggingFace)
- Tokenization (Fast tokenizers)

ğŸš€ **Ready for Use**
- English â†’ Spanish translation training
- Model checkpoint saving/loading
- TensorBoard visualization
- Flask web UI

---

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

1. **Multiple Language Pairs** - Add support for more language combinations
2. **Inference Optimization** - Quantization, ONNX export
3. **Advanced Metrics** - BLEU, METEOR, ChrF score implementations
4. **Web UI Enhancement** - Better UI/UX, batch translation
5. **Documentation** - More detailed API docs

### How to Contribute

```bash
# 1. Fork the repository
# 2. Create a feature branch
git checkout -b feature/your-feature

# 3. Make changes and commit
git add .
git commit -m "Add your feature"

# 4. Push and create PR
git push origin feature/your-feature
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [HuggingFace](https://huggingface.co/) - Datasets and Tokenizers
- [PyTorch](https://pytorch.org/) - Deep Learning Framework
- "Attention Is All You Need" - Vaswani et al., 2017

---

**Last Updated**: January 2026  
**Version**: 0.1.0  
**Status**: Active Development---

## ğŸ“ Project Structure

```
transformer/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ core/                    # ML core components
â”‚   â”‚   â”œâ”€â”€ model.py             # Transformer architecture
â”‚   â”‚   â”œâ”€â”€ dataset.py           # Data loading & preprocessing
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ web/                     # Flask web application
â”‚   â”‚   â”œâ”€â”€ app.py               # Main web app
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ monitoring/              # MLOps monitoring
â”‚   â”‚   â”œâ”€â”€ logger.py            # Structured logging
â”‚   â”‚   â”œâ”€â”€ metrics.py           # Metrics collection
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ utils/                   # Helper utilities
â”‚
â”œâ”€â”€ config/                       # Configuration management
â”‚   â”œâ”€â”€ config.py                # Main config file
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/                         # Data directory
â”‚   â””â”€â”€ tokenizers/              # Language tokenizers
â”‚
â”œâ”€â”€ scripts/                      # Standalone scripts
â”‚   â””â”€â”€ train.py                 # Training script
â”‚
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ conftest.py              # Pytest fixtures
â”‚   â”œâ”€â”€ test_model.py            # Model tests
â”‚   â”œâ”€â”€ test_data.py             # Data tests
â”‚   â”œâ”€â”€ test_monitoring.py       # Monitoring tests
â”‚   â””â”€â”€ test_model_artifacts.py  # Artifact tests
â”‚
â”œâ”€â”€ dags/                         # Airflow DAGs
â”‚   â””â”€â”€ training_pipeline_dag.py # Training orchestration
â”‚
â”œâ”€â”€ templates/                    # Flask HTML templates
â”‚   â”œâ”€â”€ base.html
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ .github/workflows/            # CI/CD pipelines
â”‚   â”œâ”€â”€ ci-cd.yml
â”‚   â””â”€â”€ model-validation.yml
â”‚
â”œâ”€â”€ main.py                       # Application entry point
â”œâ”€â”€ requirements.txt              # Production dependencies
â”œâ”€â”€ requirements_dev.txt          # Development dependencies
â”œâ”€â”€ dvc.yaml                      # DVC pipeline
â”œâ”€â”€ airflow.cfg                   # Airflow configuration
â”œâ”€â”€ Dockerfile                    # Container definition
â””â”€â”€ README.md                     # This file
```

---

## ğŸ’» Installation

### Prerequisites
- Python 3.9+
- pip or conda
- Git

### Setup

```bash
# Clone repository
git clone https://github.com/Amit95688/Transformer-From-Scratch.git
cd Transformer-From-Scratch

# Create virtual environment
python -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Verify installation
python -c "from src.core.model import build_transformer; print('âœ“ Setup successful')"
```

---

## ğŸ® Usage

### Training

```bash
python scripts/train.py
```

### Web Interface

```bash
python main.py
```

Then visit `http://localhost:5000`

### Running Tests

```bash
pip install -r requirements_dev.txt
pytest tests/ -v
```

---

## ğŸ”¬ MLOps Infrastructure

### Experiment Tracking (MLflow)

```bash
mlflow ui
```

Features:
- Hyperparameter tracking
- Metrics logging
- Model artifacts storage
- Experiment comparison

### Workflow Orchestration (Airflow)

```bash
airflow webserver --port 8080
airflow scheduler
```

DAGs:
- Daily model training
- Data validation
- Artifact versioning

### CI/CD Pipelines (GitHub Actions)

Automated workflows:
- âœ… Testing on Python 3.9-3.11
- âœ… Code linting (flake8)
- âœ… Docker build & push
- âœ… Daily model validation

### Monitoring & Logging

Structured logging with:
- JSON-formatted logs
- Real-time metrics collection
- Data drift detection
- Error rate tracking

---

## ğŸ—ï¸ Model Architecture

**Transformer Components:**
- Multi-head self-attention (8 heads)
- Feed-forward networks
- Position-wise encodings
- Residual connections
- Layer normalization

**Hyperparameters:**
```python
d_model = 128
nhead = 8
num_encoder_layers = 3
num_decoder_layers = 3
dim_feedforward = 256
dropout = 0.1
seq_length = 128
```

---

## ğŸ§ª Testing

```bash
# All tests
pytest tests/ -v

# With coverage
pytest tests/ --cov=src --cov-report=html

# Specific test
pytest tests/test_model.py -v
```

---

## ğŸ³ Docker

```bash
# Build image
docker build -t transformer:latest .

# Run container
docker run -p 5000:5000 transformer:latest python main.py
```

---

## ğŸ“š Documentation

- **[MLOPS_IMPLEMENTATION.md](MLOPS_IMPLEMENTATION.md)** - MLOps setup
- **[PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md)** - Architecture details
- **[QUICK_REFERENCE.sh](QUICK_REFERENCE.sh)** - Quick commands

---

## ğŸ¤ Contributing

```bash
# Create feature branch
git checkout -b feature/my-feature

# Make changes and run tests
pytest tests/ -v

# Commit and push
git add -A
git commit -m "Add feature: description"
git push origin feature/my-feature
```

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details

---

## ğŸ™ Acknowledgments

- PyTorch team for deep learning framework
- Hugging Face for tokenizers & datasets
- Apache Airflow for orchestration
- MLflow for experiment tracking

---

**Made with â¤ï¸ - Last Updated: January 12, 2026**
